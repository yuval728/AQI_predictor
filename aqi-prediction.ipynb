{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11151734,"sourceType":"datasetVersion","datasetId":6574811}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nmy_secret = user_secrets.get_secret(\"WANDB_API_KEY\") ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=my_secret)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom time import time\nimport warnings\nfrom sklearn.impute import KNNImputer\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.970Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 71 # for reproducibility\nSATELLITE_EXTRA_LAYER = False # for adding layer to pre-built models\nSTREET_EXTRA_LAYER = False # for adding layer to pre-built models\nBATCH_SIZE = 400 # number of samples processed in one training batch\nEPOCHS = 40 # mber of training iterations over the entire dataset\nLEARNING_RATE = 1e-2 # to update model weights\nIMG_SIZE = 224 # input image\nNUM_FROZEN_LAYERS = 0 # 0 indicates that all layers are trainable\nDROPOUT = 0.5\nLABELS = ['AQI','PM2.5', 'PM10','O3', 'CO', 'SO2', 'NO2']\n\nSATELLITE_ENCODER = \"mobilenet_v3_small\" #'vit_base_32'\nSTREET_ENCODER = \"mobilenet_v3_small\"#'vit_base_32'\nATTENTION = \"sigmoid_gated\" #\"softmax_gated\" # , \"sigmoid_gated\" ,  \"cross\" \nRUN_NAME = f'st-{SATELLITE_ENCODER}_sv-{STREET_ENCODER}_attn-{ATTENTION}'\nTAGS = [f'Satellite_{SATELLITE_ENCODER}_EL-{SATELLITE_EXTRA_LAYER}', f'Street_{STREET_ENCODER}_EL-{STREET_EXTRA_LAYER}', '7Bands', ATTENTION]\n\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run = wandb.init(\n    project=\"AQI&PollutantsDetection\",\n    name=RUN_NAME,\n    tags=TAGS,\n    resume='allow',\n    allow_val_change=True,\n    config={\n        \"SEED\": SEED,\n        \"SATELLITE_EXTRA_LAYER\": SATELLITE_EXTRA_LAYER,\n        \"STREET_EXTRA_LAYER\": STREET_EXTRA_LAYER,\n        \"BATCH_SIZE\": BATCH_SIZE,\n        \"EPOCHS\": EPOCHS,\n        \"LEARNING_RATE\": LEARNING_RATE,\n        \"IMG_SIZE\": IMG_SIZE,\n        \"NUM_FROZEN_LAYERS\": NUM_FROZEN_LAYERS,\n        \"DROPOUT\": DROPOUT,\n        \"DEVICE\": str(device),\n        \"SATELLITE_ENCODER\": SATELLITE_ENCODER,\n        \"STREET_ENCODER\": STREET_ENCODER,\n        \"LABELS\": LABELS\n    }\n)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset\n- Loads dataset: https://www.kaggle.com/datasets/adarshrouniyar/air-pollution-image-dataset-from-india-and-nepal\n- CSV consists of places and AQI measures\n- We have added an additional field for sattelite image path which was collected separately using Google earth engine.\n- Fields: Location, Filename (the street view image), Normalized_Filename (satellite image), Year, Month, Day, Hour, AQI, PM2.5, PM10, O3, CO, SO2, NO2, and AQI_Class","metadata":{}},{"cell_type":"code","source":"dataset_path = '/kaggle/input/aqi-dataset/dataset'\n# dataset_path = './dataset'\ntrain_csv = pd.read_csv(os.path.join(dataset_path, 'train_data.csv'))\nval_csv = pd.read_csv(os.path.join(dataset_path, 'val_data.csv'))\ntest_csv = pd.read_csv(os.path.join(dataset_path, 'test_data.csv'))\n\n# train_csv.interpolate(method='linear', inplace=True)\n# val_csv.interpolate(method='linear', inplace=True)\n# test_csv.interpolate(method='linear', inplace=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_df = pd.concat([train_csv, val_csv, test_csv])\n\n# Perform interpolation and KNN imputation\n# combined_df.interpolate(method='linear', inplace=True)\nknn_imputer = KNNImputer(n_neighbors=5)\ncombined_df[['SO2', 'NO2', 'O3', 'CO']] = knn_imputer.fit_transform(combined_df[['SO2', 'NO2', 'O3', 'CO']])\n\n# Split them back into separate datasets\ntrain_csv = combined_df.iloc[:len(train_csv)]\nval_csv = combined_df.iloc[len(train_csv):len(train_csv) + len(val_csv)]\ntest_csv = combined_df.iloc[len(train_csv) + len(val_csv):]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom Dataset","metadata":{}},{"cell_type":"markdown","source":"#### **1. Initialization (`__init__`)**  \n   - Takes a **CSV file** (*pandas DataFrame*) containing **filenames and labels**.  \n   - **`satellite_img_dir`** & **`street_img_dir`**: Directories where **satellite** and **street images** are stored.  \n   - **`label`**: Column name in the CSV corresponding to the **target label**.  \n   - **`satellite_transform`** & **`street_transform`**: Optional transformations (e.g., **resizing, normalization**) for images.  \n\n#### **2. Dataset Length (`__len__`)**  \n   - Returns the **total number of samples** (rows) in the CSV file.  \n\n#### **3. Get Item (`__getitem__`)**  \n   - Retrieves the **street image** and **satellite image** paths from the CSV.  \n   - Loads the images using the **`load_image()`** method.  \n   - Applies the **respective transformations** if provided.  \n   - Converts the **label** to a **PyTorch tensor** of type `float32`.  \n   - Returns a tuple: **`(street_image, satellite_image, label)`**.  \n\n#### **4. Image Loading (`load_image`)**  \n   - Opens an image using **PIL** (`Image.open(file_path)`).  \n   - Applies **transformations** if specified.  \n   - Returns the **processed image**. ðŸš€  ","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, csv_file, satellite_img_dir, street_img_dir, label, satellite_transform=None, street_transform=None):\n        self.data = csv_file\n        self.street_img_dir = street_img_dir\n        self.satellite_img_dir = satellite_img_dir\n        self.label = label\n        self.satellite_transform = satellite_transform\n        self.street_transform = street_transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        street_image_path = os.path.join(self.street_img_dir, self.data['Filename'].iloc[idx])\n        satellite_image_path = os.path.join(self.satellite_img_dir, self.data['Normalized_Filename'].iloc[idx]+'.npy' )\n        \n        street_image = self.load_image(street_image_path, self.street_transform)\n        # satellite_image = self.load_image(satellite_image_path, self.satellite_transform)\n        satellite_image = self.load_npy(satellite_image_path, self.satellite_transform)\n        labels = torch.tensor(self.data[self.label].iloc[idx], dtype=torch.float32)\n        \n        return street_image, satellite_image, labels\n        \n    \n    def load_image(self, file_path, transform=None):\n        img = Image.open(file_path)\n        if transform:\n            img = transform(img)\n        return img\n    \n    def load_npy(self, file_path, transform=None):\n        img = np.load(file_path)\n        img = torch.tensor(img, dtype=torch.float32)\n\n        # If the data is in [0, 255], scale it to [0, 1]\n        if img.max() > 1.0:  \n            img = img / 255.0  \n\n        # Ensure shape (C, H, W) if it's (H, W, C)\n        if img.ndimension() == 3 and img.shape[-1] == 7:  \n            img = img.permute(2, 0, 1)  \n        if transform:\n            img = transform(img)\n        return img\n    ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data normalization and tranformation","metadata":{}},{"cell_type":"code","source":"image_net_means = [0.485, 0.456, 0.406] \nimage_net_stds = [0.229, 0.224, 0.225]\n\nsatellite_net_means = [0.18168019890450375, 0.18805927958530722, 0.20592676343591497, 0.20806291225568016, 0.3423790143310607, 0.23654847637549638, 0.17482840221654344]\n\nsatellite_net_stds = [0.19048610465575523, 0.19615030016268702, 0.2125846014779801,  0.21476670175116374, 0.347457205638518, 0.2390436189214837, 0.17736793155031446]\n# for data transformation\ndata_transforms = {\n    'street_dev': transforms.Compose([\n        transforms.ToTensor(),  # Converts (H, W, C) to (C, H, W)\n        transforms.Normalize(tuple(image_net_means), tuple(image_net_stds))\n    ]),\n    'street_test': transforms.Compose([\n        transforms.ToTensor(),  # Converts (H, W, C) to (C, H, W)\n        transforms.Normalize(tuple(image_net_means), tuple(image_net_stds))\n    ]),\n    'satellite_dev': transforms.Compose([\n        # transforms.ToTensor(),  # Converts (H, W, C) to (C, H, W)\n        transforms.Resize((224, 224)),\n        transforms.Normalize(tuple(satellite_net_means), tuple(satellite_net_stds))\n    ]),\n    'satellite_test': transforms.Compose([\n        # transforms.ToTensor(),  # Converts (H, W, C) to (C, H, W)\n        transforms.Resize((224, 224)),\n        transforms.Normalize(tuple(satellite_net_means), tuple(satellite_net_stds))\n    ])\n}   \n    \n# image specific tranformations    \nsatellite_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    # transforms.ToTensor(),\n    transforms.RandomHorizontalFlip(),  \n    transforms.RandomVerticalFlip(),\n    transforms.Normalize(tuple(satellite_net_means), tuple(satellite_net_stds)),\n])\n\nstreet_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.RandomHorizontalFlip(), \n    transforms.Normalize(tuple(image_net_means), tuple(image_net_stds)),\n])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataloaders \n\n1. **Creating Datasets (`CustomDataset`)**  \n   - `train_dataset`: Uses **augmented transforms** (`satellite_transform`, `street_transform`).  \n   - `val_dataset` & `test_dataset`: Use **only normalization** (`data_transforms['dev']` & `data_transforms['test']`).  \n\n2. **Data Loaders (`DataLoader`)**  \n   - `train_loader`: Loads training data with **shuffling** for randomness.  \n   - `val_loader` & `test_loader`: Load validation & test data **without shuffling** for consistency.  \n\nUsage:\n- Prepares **train, validation, and test sets** for deep learning models.  \n- Uses **batch processing (`batch_size=BATCH_SIZE`)** for efficient training.  \n- **`next(iter(train_loader))`** retrieves a single batch to inspect data. ðŸš€","metadata":{}},{"cell_type":"code","source":"train_dataset = CustomDataset(train_csv, os.path.join(dataset_path, 'satellite_images'), os.path.join(dataset_path, 'All_img'), LABELS, satellite_transform, street_transform)\n\nval_dataset = CustomDataset(val_csv, os.path.join(dataset_path, 'satellite_images'), os.path.join(dataset_path, 'All_img'), LABELS, data_transforms['satellite_dev'], data_transforms['street_dev'])\n\ntest_dataset = CustomDataset(test_csv, os.path.join(dataset_path, 'satellite_images'), os.path.join(dataset_path, 'All_img'), LABELS, data_transforms['satellite_test'], data_transforms['street_test'])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n# data = next(iter(train_loader))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_loader), len(val_loader), len(test_loader)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# temp = next(iter(train_loader))\n# print(temp[0].shape, temp[1].shape, temp[2].shape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"code","source":"def loss_fn_regression(outputs, targets):\n    \"\"\"\n    Loss function for regression\n    \"\"\"\n    return nn.MSELoss()(outputs, targets)\n\n\ndef loss_fn_classification(outputs, targets):\n    \"\"\"\n    Loss function for classification\n    \"\"\"\n\n    return nn.CrossEntropyLoss()(outputs, targets)\n\n\ndef accuracy(outputs, targets):\n    \"\"\"\n    Accuracy function\n    \"\"\"\n\n    return (outputs.argmax(1) == targets).float().mean()\n\n\ndef rmse(outputs, targets):\n    \"\"\"\n    RMSE function\n    \"\"\"\n\n    return torch.sqrt(nn.MSELoss()(outputs, targets))\n\n\ndef mae(outputs, targets):\n    \"\"\"\n    MAE function\n    \"\"\"\n\n    return nn.L1Loss()(outputs, targets)\n\ndef multi_task_loss(aqi_pred, aqi_true, pm_pred, pm_true, gas_pred, gas_true, loss_fn=loss_fn_regression, weights=None):\n    \n    if weights is None:\n        weights = [1.0, 1.0, 1.0]  # Default equal weights for AQI, PM, and Gases\n\n    # print(weights)\n    loss_aqi = loss_fn(aqi_pred, aqi_true)\n    loss_pm = loss_fn(pm_pred, pm_true)\n    loss_gas = loss_fn(gas_pred, gas_true)\n    # print(loss_aqi, loss_pm, loss_gas)\n    # Weighted sum of losses\n    total_loss = weights[0] * loss_aqi + weights[1] * loss_pm + weights[2] * loss_gas\n    return total_loss\n\ndef dynamic_task_weights(y_aqi, y_pm, y_gas):\n    # Compute inverse variance (higher variance â†’ lower weight)\n    var_aqi = torch.var(y_aqi)\n    var_pm = torch.var(y_pm)\n    var_gas = torch.var(y_gas)\n\n    weights = 1 / (torch.tensor([var_aqi, var_pm, var_gas]) + 1e-6)  # Avoid division by zero\n    weights /= torch.sum(weights)  # Normalize weights to sum to 1\n    return weights.tolist()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Batch processing - forward pass\nProcesses a batch of data through the model and computes loss & metrics.  \n\n1. **Inputs:**  \n   - `data`: (street images, satellite images, targets)  \n   - `model`: Deep learning model  \n   - `device`: CPU/GPU  \n   - `is_classification`: Flag for classification vs. regression  \n\n2. **Steps:**  \n   - Move data to **device**.  \n   - Forward pass through **model**.  \n   - Compute **loss** (classification or regression).  \n   - Compute **accuracy** (only for classification).  \n   - Compute **RMSE & MAE** (for both tasks).  \n\n3. **Outputs:**  \n   - Loss, Accuracy (0 for regression), RMSE, MAE ðŸš€","metadata":{}},{"cell_type":"code","source":"def process_batch(data, model, device, is_classification):\n    \"\"\"Processes a batch and returns outputs, loss, and metrics.\"\"\"\n    street_img, satellite_img, targets = data\n    street_img, satellite_img, targets = street_img.to(device), satellite_img.to(device), targets.to(device)\n    \n    outputs = model(street_img, satellite_img)\n    \n    if is_classification:\n        loss = loss_fn_classification(outputs, targets)\n        acc = accuracy(outputs, targets).item()\n        rmse_score = 0\n        mae_score = 0\n    else:\n        aqi_pred, pm_pred, gas_pred = outputs\n        \n        target_aqi = targets[:, 0].unsqueeze(1)  # AQI (1 value)\n        target_pm = targets[:, 1:3]  # PM2.5, PM10 (2 values)\n        target_gas = targets[:, 3:]  # O3, CO, SO2, NO2 (4 values)\n\n        weights = dynamic_task_weights(target_aqi, target_pm, target_gas)\n        # print(weights)\n        \n        loss = multi_task_loss(aqi_pred, target_aqi, pm_pred, target_pm, gas_pred, target_gas, weights=weights)\n        acc = 0  # Not applicable for regression\n\n        rmse_score = multi_task_loss(aqi_pred, target_aqi, pm_pred, target_pm, gas_pred, target_gas, rmse, weights=weights).item()\n        mae_score = multi_task_loss(aqi_pred, target_aqi, pm_pred, target_pm, gas_pred, target_gas, mae, weights=weights).item()\n\n    return loss, acc, rmse_score, mae_score\n    ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train and evaluation functions\n1. **`train_fn(model, optimizer, scheduler, data_loader, device, is_classification=False)`**  \n   - Trains the model for one epoch by iterating over the training dataset, computing loss, and updating weights using backpropagation.  \n   - Tracks and prints batch-wise loss, RMSE, and MAE while applying learning rate scheduling.  \n\n2. **`eval_fn(model, data_loader, device, is_classification=False)`**  \n   - Evaluates the model on validation or test data without updating weights.  \n   - Computes and returns the average loss, accuracy (if applicable), RMSE, and MAE for the entire dataset.  \n\n3. **`train(model, optimizer, scheduler, train_loader, val_loader, test_loader, device, epochs=10, best_model_path='best_model.pth')`**  \n   - Manages the full training process across multiple epochs, tracking performance on both training and validation sets.  \n   - Saves the best-performing model based on validation loss and evaluates it on the test dataset after training. ðŸš€","metadata":{}},{"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, data_loader, device, is_classification=False):\n    \"\"\"Training function\"\"\"\n    model.train()\n    total_loss, total_acc, total_rmse, total_mae = 0, 0, 0, 0\n\n    for i, data in tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Training\"):\n        optimizer.zero_grad()\n\n        loss, acc, rmse_score, mae_score = process_batch(data, model, device, is_classification)\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # Aggregate metrics\n        total_loss += loss.item()\n        total_acc += acc\n        total_rmse += rmse_score\n        total_mae += mae_score\n        \n        wandb.log({\n            \"batch_loss\": loss.item(),\n            \"batch_acc\": acc,\n            \"batch_rmse\": rmse_score,\n            \"batch_mae\": mae_score\n        })\n\n        print(f\"Batch: {i+1}/{len(data_loader)}, Loss: {total_loss / (i+1):.4f}, RMSE: {total_rmse / (i+1):.4f}, MAE: {total_mae / (i+1):.4f}\", end='\\r')\n\n    print()  # Ensure proper formatting after tqdm\n\n    epoch_loss = total_loss / len(data_loader)\n    epoch_acc = total_acc / len(data_loader)\n    epoch_rmse = total_rmse / len(data_loader)\n    epoch_mae = total_mae / len(data_loader)\n\n    # Log epoch-wise metrics to WandB\n    wandb.log({\n        \"epoch_loss\": epoch_loss,\n        \"epoch_acc\": epoch_acc,\n        \"epoch_rmse\": epoch_rmse,\n        \"epoch_mae\": epoch_mae\n    })\n\n    return epoch_loss, epoch_acc, epoch_rmse, epoch_mae","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval_fn(model, data_loader, device, is_classification=False):\n    \"\"\"Evaluation function\"\"\"\n    model.eval()\n    total_loss, total_acc, total_rmse, total_mae = 0, 0, 0, 0\n\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Evaluating\"):\n            loss, acc, rmse_score, mae_score = process_batch(data, model, device, is_classification)\n\n            total_loss += loss.item()\n            total_acc += acc\n            total_rmse += rmse_score\n            total_mae += mae_score\n\n    val_loss = total_loss / len(data_loader)\n    val_acc = total_acc / len(data_loader)\n    val_rmse = total_rmse / len(data_loader)\n    val_mae = total_mae / len(data_loader)\n\n    # Log validation metrics to WandB\n    wandb.log({\n        \"val_loss\": val_loss,\n        \"val_acc\": val_acc,\n        \"val_rmse\": val_rmse,\n        \"val_mae\": val_mae\n    })\n\n    return val_loss, val_acc, val_rmse, val_mae","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, optimizer, scheduler, train_loader, val_loader, test_loader, device, epochs=10, best_model_path='best_model.pth'):\n    \"\"\"Main training loop\"\"\"\n    model.to(device)\n    history = {\"losses\": [], \"accuracies\": [], \"rmse_scores\": [], \"mae_scores\": []}\n\n    best_loss = np.inf\n    # best_model_path = \"best_model.pth\"\n\n    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n        train_metrics = train_fn(model, optimizer, scheduler, train_loader, device, is_classification=False)\n        val_metrics = eval_fn(model, val_loader, device, is_classification=False)\n\n        history[\"losses\"].append((train_metrics[0], val_metrics[0]))\n        history[\"accuracies\"].append((train_metrics[1], val_metrics[1]))\n        history[\"rmse_scores\"].append((train_metrics[2], val_metrics[2]))\n        history[\"mae_scores\"].append((train_metrics[3], val_metrics[3]))\n\n        print(f\"Epoch: {epoch+1}/{epochs}, Train Loss: {train_metrics[0]:.4f}, Train Acc: {train_metrics[1]:.4f}, Train RMSE: {train_metrics[2]:.4f}, Train MAE: {train_metrics[3]:.4f}, Val Loss: {val_metrics[0]:.4f}, Val Acc: {val_metrics[1]:.4f}, Val RMSE: {val_metrics[2]:.4f}, Val MAE: {val_metrics[3]:.4f}\")\n\n        # Save best model\n        if val_metrics[0] < best_loss:\n            best_loss = val_metrics[0]\n            torch.save(model.state_dict(), best_model_path)\n            # wandb.save(best_model_path) \n\n    # Load best model and evaluate on test set\n    model.load_state_dict(torch.load(best_model_path, weights_only=True))\n    test_metrics = eval_fn(model, test_loader, device, is_classification=False)\n    wandb.log({\n        \"test_loss\": test_metrics[0],\n        \"test_acc\": test_metrics[1],\n        \"test_rmse\": test_metrics[2],\n        \"test_mae\": test_metrics[3]\n    })\n    print(f\"Test Loss: {test_metrics[0]:.4f}, Test Acc: {test_metrics[1]:.4f}, Test RMSE: {test_metrics[2]:.4f}, Test MAE: {test_metrics[3]:.4f}\")\n\n    return history","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plotting metrics","metadata":{}},{"cell_type":"code","source":"def plot_metrics(losses, accuracies, rmse_scores, mae_scores, model_name=\"Model Metrics\"):\n    \"\"\"\n    Plot training metrics with a title.\n    \"\"\"\n\n    fig, ax = plt.subplots(2, 2, figsize=(20, 15))\n    fig.suptitle(model_name, fontsize=20, fontweight=\"bold\")  # Add model title\n\n    ax[0, 0].plot(losses)\n    ax[0, 0].set_title(\"Loss\")\n    ax[0, 0].legend([\"Train\", \"Val\"])\n\n    ax[0, 1].plot(accuracies)\n    ax[0, 1].set_title(\"Accuracy\")\n    ax[0, 1].legend([\"Train\", \"Val\"])\n\n    ax[1, 0].plot(rmse_scores)\n    ax[1, 0].set_title(\"RMSE\")\n    ax[1, 0].legend([\"Train\", \"Val\"])\n\n    ax[1, 1].plot(mae_scores)\n    ax[1, 1].set_title(\"MAE\")\n    ax[1, 1].legend([\"Train\", \"Val\"])\n\n    plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Base Model\n1. Initializes base based on architecture selected.\n2. Removes classification layer\n3. Adjusts first layer based on number of channels\n4. Adds final layers to extract features of image\n5. Freeze layers depending on number of layers\n6. Combine everything","metadata":{}},{"cell_type":"code","source":"class BaseEncoder(nn.Module):\n    \"\"\"\n    A flexible encoder model that supports different architectures (ResNet, EfficientNet, etc.).\n    \"\"\"\n\n    def __init__(\n        self,\n        arch=\"resnet18\",\n        pretrained=True,\n        no_channels=3,\n        dropout=0.5,\n        add_block=False,\n        num_frozen=0,\n    ):\n        super(BaseEncoder, self).__init__()\n\n        self.add_block = add_block\n        self.num_frozen = num_frozen\n\n        # Dictionary of available architectures\n        arch_dict = {\n            # ResNets\n            \"resnet18\": models.resnet18,\n            \"resnet34\": models.resnet34,\n            \"resnet50\": models.resnet50,\n            # \"resnet101\": models.resnet101,\n            # EfficientNets\n            \"efficientnet_b0\": models.efficientnet_b0,\n            \"efficientnet_b1\": models.efficientnet_b1,\n            \"efficientnet_b2\": models.efficientnet_b2,\n            # \"efficientnet_b3\": models.efficientnet_b3,\n            # \"efficientnet_b4\": models.efficientnet_b4,\n            # \"efficientnet_b5\": models.efficientnet_b5,\n            # \"efficientnet_b6\": models.efficientnet_b6,\n            # \"efficientnet_b7\": models.efficientnet_b7,\n            # MobileNets\n            \"mobilenet_v3_small\": models.mobilenet_v3_small,\n            \"mobilenet_v3_large\": models.mobilenet_v3_large,\n            # Convnexts\n            \"convnext_tiny\": models.convnext_tiny,\n            \"convnext_small\": models.convnext_small,\n            \"convnext_base\": models.convnext_base,\n            # \"convnext_large\": models.convnext_large,\n            # Vision Transformer\n            \"vit_base_16\": models.vit_b_16,\n            \"vit_base_32\": models.vit_b_32,\n            # \"vit_large_16\": models.vit_l_16,\n            # \"vit_large_32\": models.vit_l_32,\n            # \"vit_huge_14\": models.vit_h_14,\n            # Swin Transformer V2\n            \"swinv2_tiny\": models.swin_v2_t,\n            \"swinv2_small\": models.swin_v2_s,\n            \"swinv2_base\": models.swin_v2_b,\n        }\n\n        assert arch in arch_dict, (\n            f\"Unsupported architecture: {arch}. Choose from {list(arch_dict.keys())}\"\n        )\n\n        # Load the model\n        self.model = arch_dict[arch](weights=\"DEFAULT\" if pretrained else None)\n\n        if \"resnet\" in arch:\n            self.feature_dim = self.model.fc.in_features\n            self.model.fc = nn.Identity()\n        elif \"efficientnet\" in arch:\n            self.feature_dim = self.model.classifier[1].in_features\n            self.model.classifier = nn.Identity()\n        elif \"mobilenet\" in arch:\n            self.feature_dim = self.model.classifier[0].in_features\n            self.model.classifier = nn.Identity()\n        elif \"convnext\" in arch:\n            self.feature_dim = self.model.classifier[2].in_features  # [LayerNorm, Flatten, Linear]\n            self.model.classifier = self.model.classifier[:2]\n        elif \"vit\" in arch:\n            self.feature_dim = self.model.heads.head.in_features\n            self.model.heads.head = nn.Identity()\n        elif \"swin\" in arch:\n            self.feature_dim = self.model.head.in_features\n            self.model.head = nn.Identity()\n\n        if no_channels != 3:\n            if \"resnet\" in arch:\n                self.model.conv1 = nn.Conv2d(\n                    no_channels,\n                    self.model.conv1.out_channels,\n                    kernel_size=self.model.conv1.kernel_size,\n                    stride=self.model.conv1.stride,\n                    padding=self.model.conv1.padding,\n                    bias=False,\n                )\n\n            elif \"efficientnet\" in arch:\n                self.model.features[0][0] = nn.Conv2d(\n                    no_channels,\n                    self.model.features[0][0].out_channels,\n                    kernel_size=self.model.features[0][0].kernel_size,\n                    stride=self.model.features[0][0].stride,\n                    padding=self.model.features[0][0].padding,\n                    bias=False,\n                )\n\n            elif \"mobilenet\" in arch:\n                self.model.features[0][0] = nn.Conv2d(\n                    no_channels,\n                    self.model.features[0][0].out_channels,\n                    kernel_size=self.model.features[0][0].kernel_size,\n                    stride=self.model.features[0][0].stride,\n                    padding=self.model.features[0][0].padding,\n                    bias=False,\n                )\n\n            elif \"convnext\" in arch:\n                self.model.features[0][0] = nn.Conv2d(\n                    no_channels,\n                    self.model.features[0][0].out_channels,\n                    kernel_size=self.model.features[0][0].kernel_size,\n                    stride=self.model.features[0][0].stride,\n                    padding=self.model.features[0][0].padding,\n                    bias=self.model.features[0][0].bias is not None,\n                )\n\n            elif \"vit\" in arch:  \n                self.model.conv_proj = nn.Conv2d(\n                    no_channels,\n                    self.model.conv_proj.out_channels,\n                    kernel_size=self.model.conv_proj.kernel_size,\n                    stride=self.model.conv_proj.stride,\n                    padding=self.model.conv_proj.padding,\n                    bias=self.model.conv_proj.bias is not None,\n                )\n\n            elif \"swin\" in arch:\n                first_conv_layer = self.model.features[0][0]  # Access the first Conv2d layer\n                self.model.features[0][0] = nn.Conv2d(\n                    no_channels, \n                    first_conv_layer.out_channels, \n                    kernel_size=first_conv_layer.kernel_size, \n                    stride=first_conv_layer.stride, \n                    padding=first_conv_layer.padding, \n                    bias=first_conv_layer.bias is not None\n                )\n\n        if self.add_block:\n            self.addition_block = nn.Sequential(\n                nn.Linear(self.feature_dim, self.feature_dim),\n                nn.BatchNorm1d(self.feature_dim),\n                nn.Dropout(dropout),\n                nn.Linear(self.feature_dim, self.feature_dim),\n            )\n\n        # self.final_layers = nn.Sequential(\n        #     nn.Linear(self.feature_dim, 512),\n        #     nn.BatchNorm1d(512),\n        #     nn.Dropout(dropout),\n        #     nn.Linear(512, 512),\n        # )\n\n        self.freeze_layers()\n\n    def freeze_layers(self):\n        \"\"\"\n        Freeze the first `num_frozen` layers of the model.\n        \"\"\"\n        layers = list(self.model.children())\n        assert 0 <= self.num_frozen <= len(layers), (\n            f\"num_frozen should be between 0 and {len(layers)}\"\n        )\n\n        for i, child in enumerate(layers):\n            if i < self.num_frozen:\n                for param in child.parameters():\n                    param.requires_grad = False\n\n        print(f\"Number of frozen layers: {self.num_frozen}\")\n\n    def forward(self, x):\n        x = self.model(x)\n        if self.add_block:\n            x = self.addition_block(x)\n        # x = self.final_layers(x)\n        return x\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SigmoidGatedFusion2(nn.Module):\n    \"\"\"Independent Sigmoid-based gating for each input and then concatenates.\"\"\"\n    def __init__(self, in_dim1, in_dim2):\n        super(SigmoidGatedFusion, self).__init__()\n        self.attn1 = nn.Linear(in_dim1, in_dim1)  # Produces weighted features\n        self.attn2 = nn.Linear(in_dim2, in_dim2)\n\n    def forward(self, x1, x2):\n        weight1 = torch.sigmoid(self.attn1(x1))  \n        weight2 = torch.sigmoid(self.attn2(x2))  \n        fused = torch.cat((weight1 * x1, weight2 * x2), dim=1)  # Concatenate weighted features\n        return fused\n\nclass SigmoidGatedFusion(nn.Module):\n    \"\"\"Independent Sigmoid-based gating for each input and then concatenates.\"\"\"\n    def __init__(self, in_dim1, in_dim2):\n        super(SigmoidGatedFusion, self).__init__()\n        self.attn1 = nn.Linear(in_dim1, 1)  # Produces weighted features\n        self.attn2 = nn.Linear(in_dim2, 1)\n\n    def forward(self, x1, x2):\n        weight1 = torch.sigmoid(self.attn1(x1))  \n        weight2 = torch.sigmoid(self.attn2(x2))  \n        fused = torch.cat((weight1 * x1, weight2 * x2), dim=1)  # Concatenate weighted features\n        return fused\n\n\nclass SoftmaxGatedFusion(nn.Module):\n    \"\"\"Softmax-based gating where attention weights sum to 1.\"\"\"\n    def __init__(self, in_dim1, in_dim2):\n        super(SoftmaxGatedFusion, self).__init__()\n        self.gate = nn.Linear(in_dim1 + in_dim2, 2)  # Two attention scores\n\n    def forward(self, x1, x2):\n        combined_features = torch.cat((x1, x2), dim=1)  \n        weights = torch.softmax(self.gate(combined_features), dim=1)  # Softmax over two inputs\n        weighted_x1 = weights[:, 0:1] * x1\n        weighted_x2 = weights[:, 1:2] * x2\n        fused = torch.cat((weighted_x1, weighted_x2), dim=1)  # Concatenate weighted features\n        return fused\n\n# class CrossAttention(nn.Module):\n#     \"\"\"Multihead Cross Attention treating one input as query and other as key/value, .\"\"\"\n#     def __init__(self, in_dim1, in_dim2, num_heads=4):\n#         super(CrossAttention, self).__init__()\n#         self.query_proj = nn.Linear(in_dim1, in_dim1)\n#         self.key_proj = nn.Linear(in_dim2, in_dim1)\n#         self.value_proj = nn.Linear(in_dim2, in_dim1)\n#         self.attn = nn.MultiheadAttention(embed_dim=in_dim1, num_heads=num_heads, batch_first=True)\n\n#     def forward(self, x1, x2):\n#         q = self.query_proj(x1).unsqueeze(1)  # Query from x1\n#         k = self.key_proj(x2).unsqueeze(1)    # Key from x2\n#         v = self.value_proj(x2).unsqueeze(1)  # Value from x2\n#         attn_output, _ = self.attn(q, k, v)  \n#         attn_output = attn_output.squeeze(1)\n#         fused = torch.cat((attn_output, x1, x2), dim=1)  # Concatenate attention output with original features\n#         return fused\n\nclass CrossAttention(nn.Module):\n    \"\"\"Multihead Cross Attention treating one input as query and other as key/value.\"\"\"\n    def __init__(self, in_dim1, in_dim2, shared_dim=256, num_heads=4):\n        super(CrossAttention, self).__init__()\n        \n        # Project both inputs to a shared dimension\n        self.query_proj = nn.Linear(in_dim1, shared_dim)\n        self.key_proj = nn.Linear(in_dim2, shared_dim)\n        self.value_proj = nn.Linear(in_dim2, shared_dim)\n\n        # Multihead attention with consistent dimensions\n        self.attn = nn.MultiheadAttention(embed_dim=shared_dim, num_heads=num_heads, batch_first=True)\n\n    def forward(self, x1, x2):\n        # Project to shared dimension\n        q = self.query_proj(x1).unsqueeze(1)  # (batch_size, 1, shared_dim)\n        k = self.key_proj(x2).unsqueeze(1)    # (batch_size, 1, shared_dim)\n        v = self.value_proj(x2).unsqueeze(1)  # (batch_size, 1, shared_dim)\n    \n        # Attention\n        attn_output, _ = self.attn(q, k, v)\n        attn_output = attn_output.squeeze(1)\n    \n        # Ensure consistent concatenation\n        fused = torch.cat((attn_output, x1, x2), dim=1)\n    \n        # ðŸš€ Add the projection layer here\n        projection = nn.Linear(fused.shape[1], 1024).to(fused.device)\n        fused = projection(fused)\n    \n        return fused\n\n\n\ndef get_attention_module(attention_type, in_dim1, in_dim2, num_heads=4):\n    \"\"\"\n    Factory method to return different attention mechanisms for concatenated features.\n\n    Args:\n    - attention_type (str): Type of attention (\"sigmoid_gated\", \"softmax_gated\", \"cross\")\n    - in_dim1 (int): Feature dimension of input 1\n    - in_dim2 (int): Feature dimension of input 2\n    - num_heads (int): Number of heads (only for cross attention)\n\n    Returns:\n    - nn.Module: The selected attention mechanism\n    \"\"\"\n    if attention_type == \"sigmoid_gated\":\n        return SigmoidGatedFusion(in_dim1, in_dim2)\n    elif attention_type == \"softmax_gated\":\n        return SoftmaxGatedFusion(in_dim1, in_dim2)\n    elif attention_type == \"cross\":\n        return CrossAttention(in_dim1, in_dim2, num_heads)\n    else:\n        raise ValueError(f\"Unknown attention type: {attention_type}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AQI model\n1. Concats output from 2 models: 1 for satellite features, 1 for street image features\n2. Adds final layers (depending on classification, regression)\n3. Forming a single model","metadata":{}},{"cell_type":"code","source":"class AQIPrediction(nn.Module):\n    \"\"\"\n    Unified model\n    \"\"\"\n    \n    def __init__(self, satellite_model, street_model, dropout=0.5, num_classes=None):\n        \n        super(AQIPrediction, self).__init__()\n        \n        self.satellite_model = satellite_model\n        self.street_model = street_model\n        self.num_classes = num_classes\n        \n        self.feature_dim = satellite_model.feature_dim + street_model.feature_dim\n        \n        self.attention = get_attention_module(ATTENTION, satellite_model.feature_dim, street_model.feature_dim)\n        \n        self.final_layers = nn.Sequential(\n            nn.Linear(self.feature_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        )\n        if num_classes:\n            self.final_layers[-1] = nn.Linear(128, num_classes)\n        else:\n            self.aqi_head = nn.Sequential(nn.Linear(128, 1), nn.ReLU())  \n            self.pm_head = nn.Sequential(nn.Linear(128, 2), nn.ReLU())   # PM2.5 & PM10\n            self.gas_head = nn.Sequential(nn.Linear(128, 4), nn.ReLU())  # O3, CO, SO2, NO2\n  \n            \n            \n    def forward(self, street_img, satellite_img):\n        \n        street_features = self.street_model(street_img)\n        satellite_features = self.satellite_model(satellite_img)\n        # print(street_features.shape, satellite_features.shape)\n        \n        # features = torch.cat((street_features, satellite_features), dim=1)\n        features = self.attention(satellite_features, street_features)\n        \n        output = self.final_layers(features)\n        if self.num_classes:\n            return output\n            \n        return self.aqi_head(output), self.pm_head(output), self.gas_head(output)\n        ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Putting it all together\nCreating the training pipeline","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\nsatellite_encoder = BaseEncoder(arch=SATELLITE_ENCODER, no_channels=7, dropout=DROPOUT, add_block=SATELLITE_EXTRA_LAYER, num_frozen=NUM_FROZEN_LAYERS)\nstreet_encoder = BaseEncoder(arch=STREET_ENCODER, no_channels=3, dropout=DROPOUT, add_block=STREET_EXTRA_LAYER, num_frozen=NUM_FROZEN_LAYERS)\n\nmodel = AQIPrediction(satellite_encoder, street_encoder, dropout=DROPOUT, num_classes=None)\n\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n\nhistory = train(model, optimizer, scheduler, train_loader, val_loader, test_loader, device, EPOCHS, f'{RUN_NAME}_best_model.pth')\n\nplot_metrics(**history)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run.finish()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-12T15:33:58.973Z"}},"outputs":[],"execution_count":null}]}