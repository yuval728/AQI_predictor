{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "EXTRA_LAYER = True\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "IMG_SIZE = 224\n",
    "NUM_FROZEN_LAYERS = 5\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset'\n",
    "train_csv = pd.read_csv(os.path.join(dataset_path, 'train_data.csv'))\n",
    "val_csv = pd.read_csv(os.path.join(dataset_path, 'val_data.csv'))\n",
    "test_csv = pd.read_csv(os.path.join(dataset_path, 'test_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, satellite_img_dir, street_img_dir, label, satellite_transform=None, street_transform=None):\n",
    "        self.data = csv_file\n",
    "        self.street_img_dir = street_img_dir\n",
    "        self.satellite_img_dir = satellite_img_dir\n",
    "        self.label = label\n",
    "        self.satellite_transform = satellite_transform\n",
    "        self.street_transform = street_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        street_image_path = os.path.join(self.street_img_dir, self.data['Filename'].iloc[idx])\n",
    "        satellite_image_path = os.path.join(self.satellite_img_dir, self.data['Normalized_Filename'].iloc[idx]+'.jpg' )\n",
    "        \n",
    "        street_image = self.load_image(street_image_path, self.street_transform)\n",
    "        satellite_image = self.load_image(satellite_image_path, self.satellite_transform)\n",
    "        label = torch.tensor(self.data[self.label].iloc[idx], dtype=torch.float32)\n",
    "        \n",
    "        return street_image, satellite_image, label\n",
    "        \n",
    "    \n",
    "    def load_image(self, file_path, transform=None):\n",
    "        img = Image.open(file_path)\n",
    "        if transform:\n",
    "            img = transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_net_means = [0.485, 0.456, 0.406]\n",
    "image_net_stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\n",
    "    'dev': transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts (H, W, C) to (C, H, W)\n",
    "        transforms.Normalize(tuple(image_net_means), tuple(image_net_stds))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts (H, W, C) to (C, H, W)\n",
    "        transforms.Normalize(tuple(image_net_means), tuple(image_net_stds))\n",
    "    ])\n",
    "}   \n",
    "    \n",
    "    \n",
    "satellite_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Normalize(tuple(image_net_means), tuple(image_net_stds))\n",
    "])\n",
    "\n",
    "street_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.Normalize(tuple(image_net_means), tuple(image_net_stds)),\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_csv, os.path.join(dataset_path, 'satellite_images'), os.path.join(dataset_path, 'All_img'), 'AQI', satellite_transform, street_transform)\n",
    "\n",
    "val_dataset = CustomDataset(val_csv, os.path.join(dataset_path, 'satellite_images'), os.path.join(dataset_path, 'All_img'), 'AQI', data_transforms['dev'], data_transforms['dev'])\n",
    "\n",
    "test_dataset = CustomDataset(test_csv, os.path.join(dataset_path, 'satellite_images'), os.path.join(dataset_path, 'All_img'), 'AQI', data_transforms['test'], data_transforms['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 8, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = next(iter(train_loader))\n",
    "# print(temp[0].shape, temp[1].shape, temp[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseResnet18(nn.Module):\n",
    "    \"\"\"\n",
    "    Base encoder model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, no_channels=3, dropout=0.5, add_block=False, num_frozen=0):\n",
    "        \n",
    "        super(BaseResnet18, self).__init__()\n",
    "\n",
    "        self.add_block = add_block\n",
    "        self.num_frozen = num_frozen\n",
    "\n",
    "        self.model= models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        if no_channels != 3:\n",
    "            self.model.conv1 = nn.Conv2d(no_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if self.add_block:\n",
    "            self.addition_block = nn.Sequential(\n",
    "                nn.Linear(in_features=1000, out_features=1000),\n",
    "                nn.BatchNorm1d(1000),\n",
    "                # nn.LayerNorm(1000),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(in_features=1000, out_features=1000)\n",
    "            )\n",
    "    \n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=1000, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=512, out_features=512)\n",
    "        )\n",
    "        \n",
    "        self.freeze_layers()\n",
    "        \n",
    "        \n",
    "    def freeze_layers(self):\n",
    "        \"\"\"\n",
    "        Freeze the first `num_frozen` layers of the model\n",
    "        \"\"\"\n",
    "        assert 0 <= self.num_frozen <= len(list(self.model.children())), \\\n",
    "            f\"Number of frozen layers should be between 0 and {len(list(self.model.children()))}\"\n",
    "        \n",
    "        for i, child in enumerate(self.model.children()):\n",
    "            if i < self.num_frozen:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                break\n",
    "        print(f\"Number of frozen layers: {self.num_frozen}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "        if self.add_block:\n",
    "            x = self.addition_block(x)\n",
    "        x = self.final_layers(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResnetRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Regression model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, no_channels=3, dropout=0.5, add_block=False, num_frozen=0):\n",
    "        \n",
    "        super(ResnetRegression, self).__init__()\n",
    "        \n",
    "        self.encoder = BaseResnet18(no_channels=no_channels, dropout=dropout, add_block=add_block, num_frozen=num_frozen)\n",
    "        self.encoder.final_layers[3] = nn.Linear(in_features=512, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ResnetClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, no_channels=3, num_classes=3, dropout=0.5, add_block=False, num_frozen=0):\n",
    "        \n",
    "        super(ResnetClassification, self).__init__()\n",
    "        \n",
    "        self.encoder = BaseResnet18(no_channels=no_channels, dropout=dropout, add_block=add_block, num_frozen=num_frozen)\n",
    "        self.encoder.final_layers[3] = nn.Linear(in_features=512, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AQIPrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    Unified model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, satellite_model, street_model, dropout=0.5, num_classes=None):\n",
    "        \n",
    "        super(AQIPrediction, self).__init__()\n",
    "        \n",
    "        self.satellite_model = satellite_model\n",
    "        self.street_model = street_model\n",
    "        \n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(512 + 512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        if num_classes:\n",
    "            self.final_layers[-1] = nn.Linear(128, num_classes)\n",
    "            \n",
    "    def forward(self, street_img, satellite_img):\n",
    "        \n",
    "        street_features = self.street_model(street_img)\n",
    "        satellite_features = self.satellite_model(satellite_img)\n",
    "        \n",
    "        features = torch.cat((street_features, satellite_features), dim=1)\n",
    "        output = self.final_layers(features)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_regression(outputs, targets):\n",
    "    \"\"\"\n",
    "    Loss function for regression\n",
    "    \"\"\"\n",
    "    return nn.MSELoss()(outputs, targets)\n",
    "\n",
    "\n",
    "def loss_fn_classification(outputs, targets):\n",
    "    \"\"\"\n",
    "    Loss function for classification\n",
    "    \"\"\"\n",
    "\n",
    "    return nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "\n",
    "def accuracy(outputs, targets):\n",
    "    \"\"\"\n",
    "    Accuracy function\n",
    "    \"\"\"\n",
    "\n",
    "    return (outputs.argmax(1) == targets).float().mean()\n",
    "\n",
    "\n",
    "def rmse(outputs, targets):\n",
    "    \"\"\"\n",
    "    RMSE function\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.sqrt(nn.MSELoss()(outputs, targets))\n",
    "\n",
    "\n",
    "def mae(outputs, targets):\n",
    "    \"\"\"\n",
    "    MAE function\n",
    "    \"\"\"\n",
    "\n",
    "    return nn.L1Loss()(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, data_loader, device, is_classification=False):\n",
    "    \"\"\"\n",
    "    Training function\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    final_acc = 0\n",
    "    final_rmse = 0\n",
    "    final_mae = 0\n",
    "\n",
    "    for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "                \n",
    "        street_img, satellite_img, targets = data\n",
    "        street_img = street_img.to(device)\n",
    "        satellite_img = satellite_img.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if is_classification:\n",
    "            outputs = model(street_img, satellite_img)\n",
    "            loss = loss_fn_classification(outputs, targets)\n",
    "            acc = accuracy(outputs, targets)\n",
    "            final_acc += acc.item()\n",
    "        else:\n",
    "            outputs = model(street_img, satellite_img)\n",
    "            targets = targets.unsqueeze(1)\n",
    "            loss = loss_fn_regression(outputs, targets)\n",
    "\n",
    "        rmse_score = rmse(outputs, targets)\n",
    "        mae_score = mae(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        \n",
    "        final_rmse += rmse_score.item()\n",
    "        final_mae += mae_score.item()\n",
    "        \n",
    "        print(f\"Batch: {i+1}/{len(data_loader)}, Loss: {final_loss / (i+1):.4f}, RMSE: {final_rmse / (i+1):.4f}, MAE: {final_mae / (i+1):.4f}\", end='\\r')\n",
    "\n",
    "    return final_loss / len(data_loader), final_acc / len(data_loader), final_rmse / len(data_loader), final_mae / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_fn(model, data_loader, device, is_classification=False):\n",
    "    \"\"\"\n",
    "    Evaluation function\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    final_acc = 0\n",
    "    final_rmse = 0\n",
    "    final_mae = 0\n",
    "\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        street_img, satellite_img, targets = data\n",
    "        street_img = street_img.to(device)\n",
    "        satellite_img = satellite_img.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            if is_classification:\n",
    "                outputs = model(street_img, satellite_img)\n",
    "                loss = loss_fn_classification(outputs, targets)\n",
    "                acc = accuracy(outputs, targets)\n",
    "            else:\n",
    "                outputs = model(street_img, satellite_img)\n",
    "                targets = targets.unsqueeze(1)\n",
    "                loss = loss_fn_regression(outputs, targets)\n",
    "                acc = torch.tensor(0, requires_grad=False)\n",
    "\n",
    "            rmse_score = rmse(outputs, targets)\n",
    "            mae_score = mae(outputs, targets)\n",
    "\n",
    "            final_loss += loss.item()\n",
    "            final_acc += acc.item()\n",
    "            final_rmse += rmse_score.item()\n",
    "            final_mae += mae_score.item()\n",
    "\n",
    "    return final_loss / len(data_loader), final_acc / len(data_loader), final_rmse / len(data_loader), final_mae / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    satellite_encoder = BaseResnet18(no_channels=3, dropout=0.5, add_block=EXTRA_LAYER, num_frozen=NUM_FROZEN_LAYERS)\n",
    "    street_encoder = BaseResnet18(no_channels=3, dropout=0.5, add_block=EXTRA_LAYER, num_frozen=NUM_FROZEN_LAYERS)\n",
    "    model = AQIPrediction(satellite_encoder, street_encoder, dropout=0.5, num_classes=None)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    best_acc = 0\n",
    "    best_rmse = np.inf\n",
    "    best_mae = np.inf\n",
    "\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        train_loss, train_acc, train_rmse, train_mae = train_fn(model, optimizer, scheduler, train_loader, device, is_classification=False)\n",
    "        val_loss, val_acc, val_rmse, val_mae = eval_fn(model, val_loader, device, is_classification=False)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train RMSE: {train_rmse:.4f}, Train MAE: {train_mae:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        losses.append((train_loss, val_loss))\n",
    "        accuracies.append((train_acc, val_acc))\n",
    "        rmse_scores.append((train_rmse, val_rmse))\n",
    "        mae_scores.append((train_mae, val_mae))\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_acc = val_acc\n",
    "            best_rmse = val_rmse\n",
    "            best_mae = val_mae\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    print(f\"Best Val Loss: {best_loss:.4f}, Best Val Acc: {best_acc:.4f}, Best Val RMSE: {best_rmse:.4f}, Best Val MAE: {best_mae:.4f}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    test_loss, test_acc, test_rmse, test_mae = eval_fn(model, test_loader, device, is_classification=False)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "    \n",
    "    return losses, accuracies, rmse_scores, mae_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frozen layers: 5\n",
      "Number of frozen layers: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1411c38da5f8451e9ffa1e611cd6f369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84dd3d96c385464bab2c242551c9c10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 31/31, Loss: 38564.1100, RMSE: 196.2477, MAE: 167.6263\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0680f6d115a447a1954fd4c832420ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Train Loss: 38564.1100, Train Acc: 0.0000, Train RMSE: 196.2477, Train MAE: 167.6263, Val Loss: 39089.4199, Val Acc: 0.0000, Val RMSE: 197.6011, Val MAE: 168.8591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932e4497e56b42e2af5192fe0c59eabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 31/31, Loss: 38378.4477, RMSE: 195.7969, MAE: 167.5866\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ece00efdef4e869108e999cdf6e5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10, Train Loss: 38378.4477, Train Acc: 0.0000, Train RMSE: 195.7969, Train MAE: 167.5866, Val Loss: 38574.4761, Val Acc: 0.0000, Val RMSE: 196.2983, Val MAE: 168.8454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6768ac5fec6e4fe3b3558c6860be02d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses, accuracies, rmse_scores, mae_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m best_mae \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(EPOCHS)):\n\u001b[1;32m---> 21\u001b[0m     train_loss, train_acc, train_rmse, train_mae \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_classification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     val_loss, val_acc, val_rmse, val_mae \u001b[38;5;241m=\u001b[39m eval_fn(model, val_loader, device, is_classification\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_mae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_mae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 38\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m(model, optimizer, scheduler, data_loader, device, is_classification)\u001b[0m\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 38\u001b[0m final_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m final_rmse \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rmse_score\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     41\u001b[0m final_mae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mae_score\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, accuracies, rmse_scores, mae_scores = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(losses, accuracies, rmse_scores, mae_scores):\n",
    "    \"\"\"\n",
    "    Plot metrics\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "    ax[0, 0].plot(losses)\n",
    "    ax[0, 0].set_title(\"Loss\")\n",
    "    ax[0, 0].legend([\"Train\", \"Val\"])\n",
    "\n",
    "    ax[0, 1].plot(accuracies)\n",
    "    ax[0, 1].set_title(\"Accuracy\")\n",
    "    ax[0, 1].legend([\"Train\", \"Val\"])\n",
    "\n",
    "    ax[1, 0].plot(rmse_scores)\n",
    "    ax[1, 0].set_title(\"RMSE\")\n",
    "    ax[1, 0].legend([\"Train\", \"Val\"])\n",
    "\n",
    "    ax[1, 1].plot(mae_scores)\n",
    "    ax[1, 1].set_title(\"MAE\")\n",
    "    ax[1, 1].legend([\"Train\", \"Val\"])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_metrics(losses, accuracies, rmse_scores, mae_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
